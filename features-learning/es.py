#!/usr/bin/env pythonimport numpy as npfrom scipy import dot, sqrt, ones, randn, tileimport timefrom scipy import zerosimport sysimport osimport configparserfrom neuralnetwork import NNfrom policy import Policyimport pybullet_envsimport pybulletimport gymfrom random_play import rndMovementfrom predict_robotFF import supervised_netnreplications = 30                      # number of replicationsmaxevaluations = 10000000000            # max number of evaluationsprange = 0.1                            # prange range (default set through Xavier initialization)batchSize = 500                         # number of offspring (to be multiplied by 2)stepsize = 0.01                         # step sizenoiseStdDev = 0.02                      # perturbation coefficientepsilon = 1e-8                          # adam parametersbeta1 = 0.9                             # adam beta1beta2 = 0.999                           # adam beta2ninputs = 4                             # number of inputs neuronsnhiddens = 10                           # number of hiddens neuronsnoutputs = 1                            # number of output neuronsbiases = False                          # whether neurons have biasesngenes = 0                              # number of parameters (calculated on the basis of the neural architecture)# Directory of the script .pyscriptdirname = os.path.dirname(os.path.realpath(__file__))# read parameters from the configuration filedef parseConfigFile(filename):    global maxevaluations    global nreplications    global ntrials    global prange    global batchSize    global nhiddens    global ninputs    global noutputs    global biases    global environment    # The configuration file must have the following sections:    # [DEFAULT]: contains the default setting for the other sections    # [ALGO]: parameters for the algorithm    # [NET]: parameters of the neural network    # [ENV]: parameter of the environment    config = configparser.ConfigParser()    config.read(filename)    # Section DEFAULT    if (config.has_option("DEFAULT", "gymEnv")):        environment = config.get("DEFAULT", "gymEnv")    # Section ALGO    maxevaluations = config.getint("ALGO","maxevaluations")    nreplications = config.getint("ALGO","nreplications")    ntrials = config.getint("ALGO", "ntrials")    batchSize = config.getint("ALGO","batchSize")    # Section NET    nhiddens = config.getint("NET","nhiddens")    #ninputs = config.getint("NET","ninputs")    #noutputs = config.getint("NET","noutputs")    if (config.has_option("NET","biases")):        biases = config.getboolean("NET","biases")    '''    if biases:        print("Parse: network %d->%d->%d with biases" % (ninputs, nhiddens, noutputs))    else:        print("Parse: network %d->%d->%d" % (ninputs, nhiddens, noutputs))    '''# Sorting functions# Descendent sortingdef descendent_sort(vect):    # Copy of the vector    tmpv = np.copy(vect)    n = len(tmpv)    # Index list    index = np.arange(n, dtype=np.int32)    i = 0    while i < n:        # Look for maximum        maxv = tmpv[0]        maxi = 0        j = 1        while j < n:            if tmpv[j] > maxv:                maxv = tmpv[j]                maxi = j            j += 1        vect[i] = tmpv[maxi]        index[i] = maxi        i += 1        # Set invalid value        tmpv[maxi] = -999999999999.0    return vect, index# Ascendent sortingdef ascendent_sort(vect):    # Copy of the vector    tmpv = np.copy(vect)    n = len(tmpv)    # Index list    index = np.arange(n, dtype=np.int32)    i = 0    while i < n:        # Look for maximum        minv = tmpv[0]        mini = 0        j = 1        while j < n:            if tmpv[j] < minv:                minv = tmpv[j]                mini = j            j += 1        vect[i] = tmpv[mini]        index[i] = mini        i += 1        # Set invalid value        tmpv[mini] = 999999999999.0    return vect, index# average fitness of the samplesdef AverageFit(fitness):    avef = 0.0    for i in range(len(fitness)):        avef = avef + fitness[i]    avef = avef / len(fitness)    return avefdef save_var(seed,stat,bestsol):    fname = scriptdirname + "/statS" + str(seed)    stat.resize([int(len(stat) / 7), 7])    stat = stat.transpose()    np.save(fname, stat)    fname = scriptdirname + "/bestS" + str(seed)    np.save(fname, bestsol)# Main code of ES algorithm (adapted from Salimans et al., 2016)# that uses Adam optimization policydef ES(s_net,net,seed):    global ngenes    global noiseStdDev    global stepsize    global ntrials    global batchSize    global prange    global ninputs    global nhiddens    global noutputs    global biases    global epsilon    global beta1    global beta2        # Adam parameters    m = zeros(ngenes) # Mean    v = zeros(ngenes) # Variance    epsilon = 1e-8 # Constant required to avoid possible division by 0    beta1 = 0.9    beta2 = 0.999    # weights range (Xavier-2 initialization)    prange = np.sqrt(2.0 / (ngenes))    # utilities    weights = zeros(batchSize)    # fitness of samples    fitness = zeros(batchSize * 2)    # allocate and initialize the centroid (Xaxier-2' method)    #center = np.arange(ngenes, dtype=np.float64)    #center = np.random.randn(ngenes) * np.sqrt(2.0 / (ninputs + (noutputs + nhiddens)))    # Extract values from Gaussian distribution with mean 0.0 and std deviation 1.0    out = np.random.randn(ngenes).astype(np.float32)    # Scale values according to <std> and the sum of square values (i.e., it reduces values of a factor about 50.0-100.0)    out *= 1.0 / np.sqrt(np.square(out).sum(axis=0, keepdims=True))    center = out    #for g in range(ngenes):        #center[g] = (np.random.random() * (prange * 2.0) - prange)    # biases are initialized to 0.0    if biases:        for g in range(noutputs+nhiddens):                center[g] = 0.0    # Allocate utility vector    utilities = zeros(batchSize * 2)    # initialize statitiscs    stat = np.arange(0, dtype=np.float64)    centroidfit = -99    bestfit = -9999.0                       # best fitness achieved so far    bestgfit = -9999.0                      # best generalization fitness achieved so far    ceval = 0                               # current evaluation    cgen = 0                                # current generation    start_time = time.time()                # start time    nsteps = np.arange(1, dtype=np.int32)   # steps consumed    print("ES: seed %d batchSize %d stepsize %.2f noiseStdDev %.2f prange %.2f network %d->%d-%d ngenes %d" % (seed, batchSize, stepsize, noiseStdDev, prange, ninputs, nhiddens, noutputs, ngenes))    # main loop    while ceval <= maxevaluations:        cgen = cgen + 1        # randomize the traing environmental matrix#        problem.initStates(1)        # Extract samples from Gaussian distribution with mean 0.0 and standard deviation 1.0        samples = randn(batchSize,ngenes)        # create symmetric samples        symmSamples = zeros((batchSize * 2, ngenes))        for i in range(batchSize):            sampleIdx = 2 * i            symmSamples[sampleIdx,:] = samples[i,:]            symmSamples[sampleIdx + 1,:] = -samples[i,:]        # Generate offspring        offspring = tile(center.reshape(1,ngenes),(batchSize * 2,1)) + noiseStdDev * symmSamples                # evaluate offspring        for o in range(batchSize * 2):            net.set_parameters(offspring[o])            for _ in range(ntrials):                fit, steps = net.rollout(s_net = s_net)                ceval += steps                fitness[o] +=fit            fitness[o]/=ntrials        # Sort by fitness and compute weighted mean into center        fitness, index = ascendent_sort(fitness)  # maximization        for i in range(batchSize * 2):            utilities[index[i]] = i        utilities /= (batchSize * 2 - 1)        utilities -= 0.5        # The weights is the difference between the utilities of the symmetric perturbation        for i in range(batchSize):            idx = 2 * i            weights[i] = (utilities[idx] - utilities[idx + 1]) # pos - neg        # Compute the gradient        g = dot(weights, samples) # weights * samples        # Normalization over the number of samples        g /= (batchSize * 2)        # Global gradient        globalg = -g        # the stepsize        #a = stepsize # no bias correction        #if biasCorr:            # Apply bias correction        a = stepsize * sqrt(1 - beta2 ** cgen) / (1 - beta1 ** cgen)        m = beta1 * m + (1 - beta1) * globalg        v = beta2 * v + (1 - beta2) * (globalg * globalg)        # Compute the displacement        dCenter = -a * m / (sqrt(v) + epsilon)        # weight decay dCenter -= wcoeff * center        # update center        center += dCenter         # Centroid evaluation        #net.set_parameters(center)        #centroidfit,steps = net.rollout()        #ceval += steps        # We measure the generalization of the best offspring/centroid        offfit = fitness[batchSize * 2 - 1]        net.set_parameters(offspring[index[batchSize * 2 - 1]])        f=0        for _ in range(ntrials*3):            fit, steps = net.rollout(s_net=s_net,seed=42+_)            #ceval += steps            f +=fit        if (f > bestgfit):            bestgfit = f            bestgsol = np.copy(offspring[index[batchSize * 2 - 1]])            fname = scriptdirname + "/bestgS" + str(seed)            np.save(fname, bestgsol)            s_net.save_model(seed)        # collect experience from the centroid        if(cgen%1==0):            net.set_parameters(center)            centroid_actions,centroid_outputs = net.rollout_experience(s_net=s_net,seed=cgen,ntrials=1)            key_centroid = list(centroid_actions.keys())            key_store = list(s_net.store_outputs.keys())            np.random.shuffle(key_centroid)            np.random.shuffle(key_store)            #inject experience in the batch            for i in range(len(key_centroid)):                s_net.store_inputs[key_store[i]] = centroid_actions[key_centroid[i]]                s_net.store_outputs[key_store[i]] = centroid_outputs[key_centroid[i]]            # print(index)            #input("dd")            s_net.training(10)            #s_net.test(centroid_actions,centroid_outputs)        '''        if (centroidfit > offfit and centroidfit > bestfit):                bestfit = centroidfit                bestsol = np.copy(center)        '''        if (offfit > bestfit):                bestfit = offfit                bestsol = offspring[index[batchSize * 2 - 1]]        stat = np.append(stat,[ceval, bestgfit, bestfit, np.average(fitness), offfit, centroidfit, cgen])        fitness = zeros(batchSize * 2)        print('Seed %d Evals-6 %d Gen %d Bestfit %.2f Bestgfit %.2f centroid %.2f bestsam %.2f Avg %.2f weights %.2f gradient %.3f a %.3f ' % (seed, ceval , cgen, bestfit, bestgfit, centroidfit, offfit, np.average(fitness), np.average(np.absolute(center)),np.average(np.absolute(globalg)), a))        if (time.time() - start_time > 60* 10):            fname = scriptdirname + "/S" + str(seed) + ".fit"            fp = open(fname, "w")            fp.write(                'Seed %d Gen %d Evaluat %d Bestfit %.2f Bestgfit %.2f centroid %.2f bestsam %.2f Avg %.2f weights %.2f gradient %.3f Runtime %d\n' % (                seed, cgen, ceval, bestfit, bestgfit, centroidfit, offfit, np.average(fitness),                np.average(np.absolute(center)), np.average(np.absolute(globalg)), (time.time() - start_time)))            fp.close()            save_var(seed, stat, bestsol)            start_time = time.time()            fname = scriptdirname + "/S" + str(seed) + ".fit"            fp = open(fname, "w")            fp.write('Seed %d Gen %d Evaluat %d Bestfit %.2f Bestgfit %.2f centroid %.2f bestsam %.2f Avg %.2f weights %.2f gradient %.3f Runtime %d\n' % (seed, cgen, ceval, bestfit, bestgfit, centroidfit, offfit, np.average(fitness), np.average(np.absolute(center)),np.average(np.absolute(globalg)), (time.time() - start_time)))            fp.close()def test(net,seed,s_net):        fname = scriptdirname + "/bestgS" + str(seed) + ".npy"    bestgeno = np.load(fname)    print(fname)    input("here")    fitt=0    while True:        net.set_parameters(bestgeno)        fit, _ = net.rollout(s_net=s_net,render=True)        print(fit)# Main functiodef main(argv):    global nreplications    global verbose    global ngenes    global nhiddens    global ninputs    global noutputs    global biases    global environment    global ntrials    seed = 1    argc = len(argv)    filename = "configuration.ini".encode('utf-8')    doTest = False    i = 1    while (i < argc):        if (argv[i] == "-f"):            i += 1            if (i < argc):                filename = argv[i]                i += 1        elif (argv[i] == "-s"):            i += 1            if (i < argc):                seed = int(argv[i])                i += 1        elif (argv[i] == "-t"):            doTest = True            i += 1        elif (argv[i] == "-v"):            i += 1            verbose = True        else:            # We simply ignore the argument            i += 1    parseConfigFile(filename)    num_hidden =50    env = gym.make(environment)    '''    play_random = rndMovement(env)    store_inputs, store_outputs = play_random.run(10000)    store_outputs = np.array(store_outputs)    store_inputs = np.array(store_inputs)   # play_test = rndMovement(env)    #test_inputs, test_outputs = play_test.run(2000)    #test_outputs = np.array(test_outputs)    #test_inputs = np.array(test_inputs)    s_net = supervised_net(store_inputs, store_outputs)    s_net.training(20000)   # s_net.test(test_inputs=test_inputs,test_outputs=test_outputs)    #v,_=s_net.prediction(test_inputs[121])   # print(" right ",(test_outputs[121]))    #print(" mse just ", np.mean(np.square(test_inputs[121, :15]-test_outputs[121])))   # print(" mse pred ", np.mean(np.square(v-test_outputs[121])))    #input("end prova")    net = Policy(env,'mlp',s_net.num_hidden)    #net = NN(env.observation_space,env.action_space,nhiddens)    #print("Parse: network %d->%d->%d" % (net.input_size, net.hidden, net.output_size))    print("Parse: network %d->%d->%d" % (net.in_shape[0], 10, net.out_num))    #ngenes = net.netParameter()    ngenes = len(net.get_parameters())    print(ngenes)    '''    if biases:        ngenes += nhiddens + noutputs    if not doTest:        # Run the evolutionary algorithm for multiple replications        for _ in range(nreplications):            play_random = rndMovement(env)            store_inputs, store_outputs = play_random.run(1000)            s_net = supervised_net(store_inputs, store_outputs)            s_net.training(1000)            net = Policy(env, 'mlp', s_net.num_hidden)            print("Parse: network %d->%d->%d" % (net.in_shape[0], 10, net.out_num))            ngenes = len(net.get_parameters())            print(ngenes)            np.random.seed(seed)            ES(s_net,net,seed)            seed = seed + 1        pass    else:        play_random = rndMovement(env)        store_inputs, store_outputs = play_random.run(1,render=True)        s_net = supervised_net(store_inputs, store_outputs)        net = Policy(env, 'mlp', s_net.num_hidden)        s_net.restore_model(seed)        test(net,seed,s_net)if __name__ == "__main__":    main(sys.argv)